{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import onnxruntime\n",
    "import onnx\n",
    "from onnx2keras import onnx_to_keras\n",
    "from onnx_pytorch import code_gen\n",
    "import numpy as np\n",
    "import torch\n",
    "from foolbox import PyTorchModel\n",
    "from foolbox.attacks import LinfPGD\n",
    "import tensorflow as tf\n",
    "from art.attacks.evasion import FastGradientMethod\n",
    "from cleverhans.tf2.attacks.fast_gradient_method import fast_gradient_method\n",
    "from advbox import FGSM"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def sigmoid(input):\n",
    "    return 1 / (1 + np.exp(-input))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def onnx2Keras(path):\n",
    "    onnx_model = onnx.load(path)\n",
    "    model = onnx_to_keras(onnx_model, ['input_imgs', 'big_input_imgs', 'desire', 'traffic_convention', 'initial_state'],\n",
    "                          name_policy='renumerate', change_ordering=False)\n",
    "    model.save(\"models/keras\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "onnx_model = \"models/supercombo.onnx\"\n",
    "session = onnxruntime.InferenceSession(onnx_model, None)\n",
    "kerasModel = tf.keras.models.load_model(\"models/keras\", compile=False)\n",
    "pytorchModel = torch.load('models/supercombo.pt')\n",
    "pytorchModel.eval()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset = np.load('data/numpy.npz')\n",
    "\n",
    "datasize = dataset['inputImgs'].shape[0]\n",
    "\n",
    "inputImgs_data = dataset['inputImgs']\n",
    "bigInputImgs_data = dataset['bigInputImgs']\n",
    "desire_data = dataset['desire']\n",
    "trafficConvention_data = dataset['trafficConvention']\n",
    "initialState_data = dataset['initialState']\n",
    "output_data = dataset['output']\n",
    "\n",
    "plan_start_idx = 0\n",
    "plan_end_idx = 4955\n",
    "\n",
    "lanes_start_idx = plan_end_idx\n",
    "lanes_end_idx = lanes_start_idx + 528\n",
    "\n",
    "lane_lines_prob_start_idx = lanes_end_idx\n",
    "lane_lines_prob_end_idx = lane_lines_prob_start_idx + 8"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def runModel(data):\n",
    "    results = None\n",
    "    for x in range(datasize):\n",
    "        result = session.run([session.get_outputs()[0].name], {\n",
    "            session.get_inputs()[0].name: np.vsplit(data['inputImgs'], datasize)[x],\n",
    "            session.get_inputs()[1].name: np.vsplit(data['bigInputImgs'], datasize)[x],\n",
    "            session.get_inputs()[2].name: np.vsplit(data['desire'], datasize)[x],\n",
    "            session.get_inputs()[3].name: np.vsplit(data['trafficConvention'], datasize)[x],\n",
    "            session.get_inputs()[4].name: np.vsplit(data['initialState'], datasize)[x],\n",
    "        })\n",
    "        result = result[0]\n",
    "        lane_lines_prob = result[:, lane_lines_prob_start_idx:lane_lines_prob_end_idx:2]\n",
    "        lane_lines_prob = sigmoid(lane_lines_prob)\n",
    "        if np.any(results):\n",
    "            results = np.concatenate((results, lane_lines_prob), axis=0)\n",
    "        else:\n",
    "            results = lane_lines_prob\n",
    "    probabilityPoint = 0.7\n",
    "    probability = (results >= probabilityPoint).astype(int)\n",
    "    accuracy = np.sum(probability == output_data) /\\\n",
    "               (len(output_data) * len(output_data[0]))\n",
    "    return accuracy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Hieronder zullen alle tools getest worden.\n",
    "\n",
    "Testing tool –Adversial Robustness 360 Toolbox–"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "try:\n",
    "    estimator = ImageSegmentation(model=kerasModel, use_logits=False)\n",
    "    attack = FastGradientMethod(estimator=estimator, eps=0.2)\n",
    "    x_test_adv = attack.generate(x={\"input_imgs\": inputImgs_data,\n",
    "                                    \"big_input_imgs\": bigInputImgs_data,\n",
    "                                    \"desire\": desire_data,\n",
    "                                    \"traffic_convention\": trafficConvention_data,\n",
    "                                    \"initial_state\": initialState_data})\n",
    "    accuracy = runModel(x_test_adv)\n",
    "    print(\"Accuracy on adversarial test examples: {}%\".format(accuracy))\n",
    "except Exception as e: print(\"Image Segmentation does not exist\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Testing tool –Cleverhans–"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "try:\n",
    "    x_fgm = fast_gradient_method(kerasModel, {\"input_imgs\": inputImgs_data,\n",
    "                                    \"big_input_imgs\": bigInputImgs_data,\n",
    "                                    \"desire\": desire_data,\n",
    "                                    \"traffic_convention\": trafficConvention_data,\n",
    "                                    \"initial_state\": initialState_data}, 0.2, np.inf)\n",
    "    accuracy = runModel(x_fgm)\n",
    "    print(\"Accuracy on FGM adversarial examples:{}%\".format(accuracy))\n",
    "except Exception as e: print(\"Unsupported type dict expected Tensor\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Testing tool –Foolbox–"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "try:\n",
    "    model = \"model/supercombo.pt\"\n",
    "    fmodel = PyTorchModel(model, bounds=(0, 1))\n",
    "\n",
    "    attack = LinfPGD()\n",
    "    _, advs, success = attack(fmodel, {\"input_imgs\": inputImgs_data,\n",
    "                                    \"big_input_imgs\": bigInputImgs_data,\n",
    "                                    \"desire\": desire_data,\n",
    "                                    \"traffic_convention\": trafficConvention_data,\n",
    "                                    \"initial_state\": initialState_data}, epsilons=[0.2])\n",
    "    accuracy = runModel(advs)\n",
    "    print(\"Accuracy on FGM adversarial examples:{}%\".format(accuracy))\n",
    "except Exception as e: print(\"Attack expected Images and Labels\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Testing tool –AdversarialBox–"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "try:\n",
    "    attack = FGSM(kerasModel)\n",
    "    attack_config = {\"epsilons\": 1, \"epsilons_max\": 10, \"epsilon_steps\": 1, \"steps\": 100}\n",
    "    adversary = attack({\"input_imgs\": inputImgs_data,\n",
    "                                    \"big_input_imgs\": bigInputImgs_data,\n",
    "                                    \"desire\": desire_data,\n",
    "                                    \"traffic_convention\": trafficConvention_data,\n",
    "                                    \"initial_state\": initialState_data}, **attack_config)\n",
    "except Exception as e: print(\"Attack expected an image but got a Dict\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "try:\n",
    "    model = \"model/supercombo.pt\"\n",
    "    fmodel = PyTorchModel(model, bounds=(0, 1))\n",
    "\n",
    "    attack = LinfPGD()\n",
    "    _, advs, success = attack(fmodel, {\"input_imgs\": inputImgs_data,\n",
    "                                    \"big_input_imgs\": bigInputImgs_data,\n",
    "                                    \"desire\": desire_data,\n",
    "                                    \"traffic_convention\": trafficConvention_data,\n",
    "                                    \"initial_state\": initialState_data}, epsilons=[0.2])\n",
    "    accuracy = runModel(advs)\n",
    "    print(\"Accuracy on FGM adversarial examples:{}%\".format(accuracy))\n",
    "except Exception as e: print(\"Attack expected Images and Labels\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Testing tool –AdversarialBox–"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "try:\n",
    "    attack = FGSM(kerasModel)\n",
    "    attack_config = {\"epsilons\": 1, \"epsilons_max\": 10, \"epsilon_steps\": 1, \"steps\": 100}\n",
    "    adversary = attack({\"input_imgs\": inputImgs_data,\n",
    "                                    \"big_input_imgs\": bigInputImgs_data,\n",
    "                                    \"desire\": desire_data,\n",
    "                                    \"traffic_convention\": trafficConvention_data,\n",
    "                                    \"initial_state\": initialState_data}, **attack_config)\n",
    "except Exception as e: print(\"Attack expected an image but got a Dict\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "expected model to be a torch.nn.Module instance",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[1;32mIn [11]\u001B[0m, in \u001B[0;36m<cell line: 3>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# try:\u001B[39;00m\n\u001B[0;32m      2\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel/supercombo.pt\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m----> 3\u001B[0m fmodel \u001B[38;5;241m=\u001B[39m \u001B[43mPyTorchModel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbounds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      5\u001B[0m attack \u001B[38;5;241m=\u001B[39m LinfPGD()\n\u001B[0;32m      6\u001B[0m _, advs, success \u001B[38;5;241m=\u001B[39m attack(fmodel, {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minput_imgs\u001B[39m\u001B[38;5;124m\"\u001B[39m: inputImgs_data,\n\u001B[0;32m      7\u001B[0m                                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbig_input_imgs\u001B[39m\u001B[38;5;124m\"\u001B[39m: bigInputImgs_data,\n\u001B[0;32m      8\u001B[0m                                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdesire\u001B[39m\u001B[38;5;124m\"\u001B[39m: desire_data,\n\u001B[0;32m      9\u001B[0m                                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtraffic_convention\u001B[39m\u001B[38;5;124m\"\u001B[39m: trafficConvention_data,\n\u001B[0;32m     10\u001B[0m                                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minitial_state\u001B[39m\u001B[38;5;124m\"\u001B[39m: initialState_data}, epsilons\u001B[38;5;241m=\u001B[39m[\u001B[38;5;241m0.2\u001B[39m])\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\TrustworthyAI\\lib\\site-packages\\foolbox\\models\\pytorch.py:31\u001B[0m, in \u001B[0;36mPyTorchModel.__init__\u001B[1;34m(self, model, bounds, device, preprocessing)\u001B[0m\n\u001B[0;32m     28\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\n\u001B[0;32m     30\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(model, torch\u001B[38;5;241m.\u001B[39mnn\u001B[38;5;241m.\u001B[39mModule):\n\u001B[1;32m---> 31\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mexpected model to be a torch.nn.Module instance\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     33\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m model\u001B[38;5;241m.\u001B[39mtraining:\n\u001B[0;32m     34\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m warnings\u001B[38;5;241m.\u001B[39mcatch_warnings():\n",
      "\u001B[1;31mValueError\u001B[0m: expected model to be a torch.nn.Module instance"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    model = \"model/supercombo.pt\"\n",
    "    fmodel = PyTorchModel(model, bounds=(0, 1))\n",
    "\n",
    "    attack = LinfPGD()\n",
    "    _, advs, success = attack(fmodel, {\"input_imgs\": inputImgs_data,\n",
    "                                    \"big_input_imgs\": bigInputImgs_data,\n",
    "                                    \"desire\": desire_data,\n",
    "                                    \"traffic_convention\": trafficConvention_data,\n",
    "                                    \"initial_state\": initialState_data}, epsilons=[0.2])\n",
    "    accuracy = runModel(advs)\n",
    "    print(\"Accuracy on FGM adversarial examples:{}%\".format(accuracy))\n",
    "except Exception as e: print(\"Attack expected Images and Labels\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Testing tool –AdversarialBox–"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "try:\n",
    "    attack = FGSM(kerasModel)\n",
    "    attack_config = {\"epsilons\": 1, \"epsilons_max\": 10, \"epsilon_steps\": 1, \"steps\": 100}\n",
    "    adversary = attack({\"input_imgs\": inputImgs_data,\n",
    "                                    \"big_input_imgs\": bigInputImgs_data,\n",
    "                                    \"desire\": desire_data,\n",
    "                                    \"traffic_convention\": trafficConvention_data,\n",
    "                                    \"initial_state\": initialState_data}, **attack_config)\n",
    "except Exception as e: print(\"Attack expected an image but got a Dict\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}